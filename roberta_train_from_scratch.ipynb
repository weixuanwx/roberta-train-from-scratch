{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "roberta_train_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukx8vMBJ5tQi"
      },
      "source": [
        "# This notebook shows how we can train a Language Model using your own pre-tokenizer from scratch using Hugging Face\n",
        "Most of the code below is not original code written by me. This notebook is just a simple and convenient collection of the relevant articles I found while training my language model for my pre-tokenized dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oICcKZCPyKwA"
      },
      "source": [
        "# Ensuring that we are utilising the GPU hardward on google colab and the correct drive is mounted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_iPyprStV44",
        "outputId": "04a35987-4def-45d6-a16f-1a36825f9b3a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov  6 05:54:06 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFubrkhtvRy",
        "outputId": "c55c2a78-d5fd-4f97-c174-e8d420fba1db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev4g-gDIu9Ev",
        "outputId": "e3874634-0544-40a0-f724-ee62361a6c3a"
      },
      "source": [
        "cd '/content/drive/My Drive/data_science_projects/transformer_adapters'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data_science_projects/transformer_adapters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6crY83KGTbAD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6-h93JMyQcm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkb8_Y4kpz7o"
      },
      "source": [
        "# 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwS_97yoNS1",
        "outputId": "f1cd8ce9-e843-492c-897e-ee7a85dbd96c"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.6.0\n",
            "Uninstalling tensorflow-2.6.0:\n",
            "  Successfully uninstalled tensorflow-2.6.0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-m5wiesj8\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-m5wiesj8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 84.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3101542 sha256=92ad62e6e4c701d2ea4a17071491e0bb87c28b9c2b77435ee45f32d083585d42\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5g7thu2u/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n",
            "tokenizers                    0.10.3\n",
            "transformers                  4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayUwSz89oU96"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VxtepkZgx2y",
        "outputId": "0dbb3c65-d15a-4b0e-bd6d-2384f8bf80db"
      },
      "source": [
        "# Check again if GPU is being used\n",
        "\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuaoMWwhp8er"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPUNqTgoWFc",
        "outputId": "df87fe43-20a2-4f3b-d123-7399b71ba671"
      },
      "source": [
        "!ls -l data/ | head -n 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1801605\n",
            "-rw------- 1 root root  14427434 Oct 22 00:06 all_10.csv\n",
            "-rw------- 1 root root  15009474 Oct 22 00:06 all_11.csv\n",
            "-rw------- 1 root root  14769213 Oct 22 00:06 all_12.csv\n",
            "-rw------- 1 root root  15624435 Oct 22 00:06 all_13.csv\n",
            "-rw------- 1 root root  14087597 Oct 22 00:06 all_14.csv\n",
            "-rw------- 1 root root  14345086 Oct 22 00:06 all_15.csv\n",
            "-rw------- 1 root root  15137480 Oct 22 00:06 all_16.csv\n",
            "-rw------- 1 root root  15156153 Oct 22 00:06 all_17.csv\n",
            "-rw------- 1 root root  15345289 Oct 22 00:06 all_18.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6x9ofAEytf2"
      },
      "source": [
        "import os, re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWnDY1CnoWJ3"
      },
      "source": [
        "files = [f\"data/{filename}\" for filename in os.listdir(\"data/\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thpV9s5moWPU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xeo4kv1DCzuT"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvwyWG01GH_L"
      },
      "source": [
        "prepared_data = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46KtWvOeCyUV"
      },
      "source": [
        "if not prepared_data:\n",
        "  import pandas as pd\n",
        "  merged_df = pd.DataFrame()\n",
        "  for file in files[:-1]:\n",
        "    df = pd.read_csv(file,header=None)\n",
        "    merged_df = pd.concat([merged_df, df])\n",
        "  dev_df = pd.read_csv(files[-1],header=None)\n",
        "  \n",
        "  merged_df.to_csv('data/train.txt', header=None, index=None, sep=' ', mode='a')\n",
        "  dev_df.to_csv('data/dev.txt', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r_ip_KSDV4M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZ-FfeTEz64"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLMPWZDNpvFj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F-BmmlkqGJ-"
      },
      "source": [
        "# Tokenize and Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiCZriUuBtGf"
      },
      "source": [
        "train_tokenizer = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edyvPE0ypvK_"
      },
      "source": [
        "if train_tokenizer:\n",
        "\n",
        "  from tokenizers import Tokenizer\n",
        "  from tokenizers.models import WordPiece\n",
        "\n",
        "  my_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYtBcn9xZRwH"
      },
      "source": [
        "if train_tokenizer:\n",
        "\n",
        "  from tokenizers import normalizers\n",
        "  from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
        "\n",
        "  my_tokenizer.normalizer = normalizers.Sequence([Lowercase()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eKvUfxxO6SD"
      },
      "source": [
        "if train_tokenizer:\n",
        "  \n",
        "  from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
        "\n",
        "  # For example, if you would like to split by just a vertical slash\n",
        "  pre_tokenizer = CharDelimiterSplit('|')\n",
        "\n",
        "  my_tokenizer.pre_tokenizer = pre_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR5ItrUQ1lOn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97I2hHq7c53w"
      },
      "source": [
        "if train_tokenizer:\n",
        "\n",
        "  from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "  my_tokenizer.post_processor = TemplateProcessing(\n",
        "      single=\"[CLS] $A [SEP]\",\n",
        "      pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "      special_tokens=[\n",
        "          (\"[CLS]\", 1),\n",
        "          (\"[SEP]\", 2),\n",
        "      ],\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEt6CI8-c56O"
      },
      "source": [
        "if train_tokenizer:\n",
        "  from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "  VOCAB = 9999 # if you know the vocab size before hand or you could just query from your dataset\n",
        "\n",
        "  trainer = WordPieceTrainer(\n",
        "      vocab_size=VOCAB, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "  )\n",
        "  my_tokenizer.train(files, trainer)\n",
        "\n",
        "  my_tokenizer.save(\"data/cust-tokenizer-data.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5shMdwedJsW"
      },
      "source": [
        "if not train_tokenizer:\n",
        "  from transformers import PreTrainedTokenizerFast\n",
        "  my_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"data/cust-tokenizer-data.json\")\n",
        "\n",
        "  # Add the below special tokens to make use of Roberta MLM training. Have to do so because we are using a simple pre-tokenizer\n",
        "  my_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'mask_token': '[MASK]'})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJpCrChyc58r",
        "outputId": "218f22d4-9c00-4aa6-fdc6-39a2df9702f6"
      },
      "source": [
        "output = my_tokenizer.encode(\"python|.net|financial reporting|accountant's report\")\n",
        "print(output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1202, 1118, 1004, 6681, 6624, 3021, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFw3hMUweeD"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMuZxiNqR0TZ",
        "outputId": "fc51b46a-4bb4-47e8-c96a-dc811d06452e"
      },
      "source": [
        "\n",
        "output = my_tokenizer.encode_plus(\"python|.net|financial reporting|accountant's report\", max_length = 128, truncation=True, padding=True)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [1, 1202, 1118, 1004, 6681, 6624, 3021, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKKB0h12IqTb"
      },
      "source": [
        "VOCABSIZE = my_tokenizer.vocab_size # if using PreTrainedTokenizerFast\n",
        "MAX_LEN = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqgXnMyJIqQc",
        "outputId": "9006229f-0fd0-412a-bf47-4e5402b55455"
      },
      "source": [
        "# test if encode_plus works\n",
        "x = my_tokenizer.encode_plus(\"python|.net|financial reporting|accountant's report\", max_length = 128, truncation=True, padding=True) # if using PreTrainedTokenizerFast\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [1, 1202, 1118, 1004, 6681, 6624, 3021, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbLE4bxDwOpT",
        "outputId": "e49da430-82ec-48ea-8307-d70d56b2794a"
      },
      "source": [
        "# initialize our model using the configuration file. \n",
        "# As we are training from scratch, we initialize from a config that defines the architecture of the model but not restoring previously trained weights. \n",
        "# The weights will be randomly initialized.\n",
        "\n",
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "# Set a configuration for our RoBERTa model\n",
        "config = RobertaConfig(\n",
        "    vocab_size=VOCABSIZE,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "# Initialize the model from a configuration without pretrained weights\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "print('Num parameters: ',model.num_parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num parameters:  48721777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0T5_hXcS2ry"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"data/train.txt\",header=None)\n",
        "eval_df = pd.read_csv(\"data/dev.txt\",header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "8cwHRh4tTSzm",
        "outputId": "3ac04c53-bd51-45f4-aeb5-e7a6aa05c560"
      },
      "source": [
        "print(train_df.shape)\n",
        "print(eval_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1692497, 1)\n",
            "(48858, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>liaison</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>construction industry|business finance|constru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c++|c#|software development life cycle (sdlc)|...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>healthcare|culinary arts|investment|business p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>project management|project management|supply c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0                                            liaison\n",
              "1  construction industry|business finance|constru...\n",
              "2  c++|c#|software development life cycle (sdlc)|...\n",
              "3  healthcare|culinary arts|investment|business p...\n",
              "4  project management|project management|supply c..."
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5kJMwKwOtz"
      },
      "source": [
        "\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df_series, tokenizer):\n",
        "        ## encode batch encodes and converts all the data into the tokenized object which contain other information such as tokens etc which takes up RAM\n",
        "        ## by doing a ugly loop and only taking the ids which are integers, RAM consumption is much lower\n",
        "        self.examples = []\n",
        "        for example in df_series:\n",
        "            x = tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
        "            self.examples += [x.input_ids]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i])\n",
        "      \n",
        "# Create the train and evaluation dataset\n",
        "train_dataset = CustomDataset(train_df[0], my_tokenizer)\n",
        "eval_dataset = CustomDataset(eval_df[0], my_tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yxAfYgVwOwf"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Define the Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "\n",
        "# need to add mask token to tokenizer to train with masked language modeling (mlm) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWrl_p4dwOzW"
      },
      "source": [
        "OUTPUT_DIR = \"models/robertamaskedlm/output\" #@param {type: \"string\"}\n",
        "\n",
        "TRAIN_BATCH_SIZE = 16  #@param [8, 16, 32] # input batch size for training (default: 64)\n",
        "VALID_BATCH_SIZE = 8  #@param [8, 16, 32] # input batch size for testing (default: 1000)\n",
        "TRAIN_EPOCHS = 20  #@param [10, 15, 20] # number of epochs to train (default: 10)\n",
        "LEARNING_RATE = 1e-3  #@param [1e-3, 1e-4, 1e-5] # learning rate (default: 0.001)\n",
        "WEIGHT_DECAY = 0.01  #@param [0.001,0.01,0.05] \n",
        "SEED = 999   #@param {type: \"integer\"} # random seed (default: 999)\n",
        "MAX_LEN = 128 #@param {type: \"integer\"}\n",
        "SUMMARY_LEN = 7 #@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-jP9vPfCwO18"
      },
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    num_train_epochs=TRAIN_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
        "    save_steps=8192,\n",
        "    #eval_steps=4096,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "# Create the trainer for our model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    #prediction_loss_only=True,\n",
        ")\n",
        "# Train the model\n",
        "# trainer.train(OUTPUT_DIR+\"/\"+\"checkpoint-99999\") # to continue from check point if training is done in one sitting\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCH8s80LjCLr"
      },
      "source": [
        "trainer.save_model(OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDbNNChGKmIm"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TW6rvQPpZup"
      },
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRfjwPxN3i4r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnHz6qyL3oQI"
      },
      "source": [
        "# Check Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1YspLNk37jx"
      },
      "source": [
        "model_folder = \"models/robertamaskedlm/output\"\n",
        "tokenizer_folder = \"data/cust-tokenizer-data.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPXEOVR-3pVR"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "my_tokenizer = Tokenizer.from_file(tokenizer_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kULTSTNN1L2X"
      },
      "source": [
        "# AutoModel is a generic model class that will be instantiated as one of the base model classes of the library when created with the AutoModel.from_pretrained(pretrained_model_name_or_path) or the AutoModel.from_config(config) class methods.\n",
        "# This class cannot be instantiated using __init__() (throws an error).\n",
        "\n",
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(model_folder, output_hidden_states=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmr5pni12btm"
      },
      "source": [
        "encoded = my_tokenizer.encode(\"j2ee|java|python|.net|financial reporting|accountant's report\")\n",
        "input_ids = torch.tensor(encoded.ids).unsqueeze(0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_HeBuQMADAy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHEBTWaK4Ktw",
        "outputId": "3ad60197-5a23-4098-a9e5-5e4748655d12"
      },
      "source": [
        "print(encoded.tokens)\n",
        "print(encoded.word_ids)\n",
        "print(len(encoded.word_ids))\n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'j2ee', 'java', 'python', '.net', 'financial reporting', 'accountant', \"##'s \", '##report', '[SEP]']\n",
            "[None, 0, 1, 2, 3, 4, 5, 5, 5, None]\n",
            "10\n",
            "tensor([[   1, 1792,  540, 1202, 1118, 1004, 6681, 6624, 3021,    2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyFDzLAn2mtE"
      },
      "source": [
        "with torch.no_grad():\n",
        "  output = model(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wca6odmY5s8o",
        "outputId": "183a1d89-b77b-4dbb-9c09-2bc921d0a8a1"
      },
      "source": [
        "print(output['last_hidden_state'].shape)\n",
        "output['last_hidden_state']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10, 768])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.1572, -0.0595, -0.3193,  ...,  1.0063, -0.1634, -0.7406],\n",
              "         [-0.9416, -0.1351, -0.4944,  ...,  0.3276, -0.6364,  0.0367],\n",
              "         [-0.2532, -0.4811,  0.1043,  ...,  0.4378, -0.0901,  0.0998],\n",
              "         ...,\n",
              "         [ 0.8555, -0.1407, -0.3859,  ..., -0.1166,  0.4961,  0.0316],\n",
              "         [-0.3628,  0.0248, -0.9519,  ...,  0.3995, -0.1347,  0.0526],\n",
              "         [-0.5398, -0.1964, -1.1223,  ...,  0.4968,  1.0257,  0.4972]]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e7AY2zx5U7g",
        "outputId": "c9172600-4dfe-4a84-b865-13aa7898f199"
      },
      "source": [
        "# output['hidden_states'] is a tuple of outputs equal to the number of layers. Each layer's outputs is of shape (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "print(len(output['hidden_states']))\n",
        "print(output['hidden_states'][0].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "torch.Size([1, 10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EjsSOwF2yud",
        "outputId": "22e561ec-b09d-4794-a4cb-29db6c6b492b"
      },
      "source": [
        "len(output['last_hidden_state']) + len(output['hidden_states'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxt10CCzCNdu",
        "outputId": "05b4d48c-e968-4866-83c4-f5879391ff2b"
      },
      "source": [
        "# the last_hidden_state is the last layer's output of all the hidden_states\n",
        "print(output['last_hidden_state'])\n",
        "print(output.hidden_states[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-1.1572, -0.0595, -0.3193,  ...,  1.0063, -0.1634, -0.7406],\n",
            "         [-0.9416, -0.1351, -0.4944,  ...,  0.3276, -0.6364,  0.0367],\n",
            "         [-0.2532, -0.4811,  0.1043,  ...,  0.4378, -0.0901,  0.0998],\n",
            "         ...,\n",
            "         [ 0.8555, -0.1407, -0.3859,  ..., -0.1166,  0.4961,  0.0316],\n",
            "         [-0.3628,  0.0248, -0.9519,  ...,  0.3995, -0.1347,  0.0526],\n",
            "         [-0.5398, -0.1964, -1.1223,  ...,  0.4968,  1.0257,  0.4972]]])\n",
            "tensor([[[-1.1572, -0.0595, -0.3193,  ...,  1.0063, -0.1634, -0.7406],\n",
            "         [-0.9416, -0.1351, -0.4944,  ...,  0.3276, -0.6364,  0.0367],\n",
            "         [-0.2532, -0.4811,  0.1043,  ...,  0.4378, -0.0901,  0.0998],\n",
            "         ...,\n",
            "         [ 0.8555, -0.1407, -0.3859,  ..., -0.1166,  0.4961,  0.0316],\n",
            "         [-0.3628,  0.0248, -0.9519,  ...,  0.3995, -0.1347,  0.0526],\n",
            "         [-0.5398, -0.1964, -1.1223,  ...,  0.4968,  1.0257,  0.4972]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk0-bKEU_HdG"
      },
      "source": [
        "# Convert the hidden states into a word embedding that is contextualised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZqTXFxB-vH4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_word_idx(sent: str, word: str):\n",
        "  return sent.split(\"|\").index(word)\n",
        " \n",
        " \n",
        "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
        "  input_ids = torch.tensor(encoded.ids).unsqueeze(0) \n",
        "  with torch.no_grad():\n",
        "      output = model(input_ids)\n",
        "\n",
        "  # Get all hidden states\n",
        "  states = output.hidden_states\n",
        "  # Stack and sum all requested layers\n",
        "  output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
        "  # Only select the tokens that constitute the requested word\n",
        "  word_tokens_output = output[token_ids_word]\n",
        "\n",
        "  return word_tokens_output.mean(dim=0)\n",
        " \n",
        " \n",
        "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
        "  encoded = tokenizer.encode(sent)\n",
        "  # get all token idxs that belong to the word of interest\n",
        "  token_ids_word = np.where(np.array(encoded.word_ids) == idx)\n",
        "\n",
        "  return get_hidden_states(encoded, token_ids_word, model, layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP_-t4VC_FU"
      },
      "source": [
        "layers = None\n",
        "layers = [-2, -1] if layers is None else layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLn2KoeBBA8C"
      },
      "source": [
        "### The entity \"Risk Management\" is contextualised differently depending on its surrounding entities in the 'sentence'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-SiFdDtEEtG",
        "outputId": "635e0fa7-1247-45a8-ec4d-591be63d7e79"
      },
      "source": [
        "sent = \"credit risk|risk management\" \n",
        "idx = get_word_idx(sent, \"risk management\")\n",
        "word_embedding_1 = get_word_vector(sent, idx, my_tokenizer, model, layers)\n",
        "\n",
        "sent = \"cyber security|risk management\" \n",
        "idx = get_word_idx(sent, \"risk management\")\n",
        "word_embedding_2 = get_word_vector(sent, idx, my_tokenizer, model, layers)\n",
        "\n",
        "print(cos(word_embedding_1,word_embedding_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9464)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN4HfVk06Pq7"
      },
      "source": [
        "# Reference\n",
        "- https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=ri2BIQKqjfHm\n",
        "- https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
        "- https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c\n",
        "- https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QouOBiXN6RDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}